{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b720e-3853-4381-a6ca-1539cd9d1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm RAG í”ŒëŸ¬ê·¸ì¸ ì„¤ì •(ì„ë² ë”©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d923a60-c928-4f3c-8683-2e6464c0ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ede17c-965c-4439-ab04-29e26f6bc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ìˆ˜ì •\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document # LangChainì˜ ê¸°ë³¸ ë¬¸ì„œ í˜•ì‹ì„ import\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc75f3-cde9-44b8-8ed4-bf2ef8ecdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag_pipeline_chroma(file_path: str, embedding_model_name: str, persist_directory: str):\n",
    "    \"\"\"\n",
    "    CSV íŒŒì¼ë¡œë¶€í„° ChromaDB RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³  ê²€ìƒ‰ê¸°(Retriever)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # 1. Embed: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì´ì „ê³¼ ë™ì¼)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    print(\"âœ… ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # 2. ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë˜ëŠ” ë¡œë“œ\n",
    "    # persist_directoryì— DBê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìƒˆë¡œ ë§Œë“¤ì§€ ì•Šê³  ë°”ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    if os.path.exists(persist_directory):\n",
    "        print(\"âœ… ê¸°ì¡´ ChromaDBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\")\n",
    "        vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    else:\n",
    "        print(\"âš ï¸ ê¸°ì¡´ ChromaDBê°€ ì—†ìœ¼ë¯€ë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        # 1. Pandasë¡œ CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        print(f\"âœ… Pandasë¡œ CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. (ì´ {len(df)}ê°œ í–‰)\")\n",
    "\n",
    "        # 2. ê° í–‰ì„ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•˜ê³ , LangChain 'Document' ê°ì²´ë¡œ ë§Œë“¤ê¸°\n",
    "        langchain_documents = []\n",
    "        for timestamp, values in df.iterrows():\n",
    "            # ì§ì ‘ ë§Œë“œì‹  ìì—°ì–´ ë³€í™˜ ë¡œì§\n",
    "            sentence = f\"'{timestamp}'ì˜ \"\n",
    "            value_strings = [f\"'{col}'ëŠ” {val}\" for col, val in values.items()]\n",
    "            sentence += \", \".join(value_strings)\n",
    "            \n",
    "            # ë³€í™˜ëœ ë¬¸ì¥ì„ page_contentë¡œ í•˜ëŠ” Document ê°ì²´ ìƒì„±\n",
    "            # metadataì— ì›ë³¸ ì‹œê°„ ì •ë³´ë¥¼ ë„£ì–´ë‘ë©´ ë‚˜ì¤‘ì— ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "            doc = Document(\n",
    "                page_content=sentence,\n",
    "                metadata={\"timestamp\": timestamp}\n",
    "            )\n",
    "            langchain_documents.append(doc)\n",
    "        \n",
    "        print(f\"âœ… {len(langchain_documents)}ê°œì˜ ìì—°ì–´ ë¬¸ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # Store: ChromaDBë¥¼ ìƒì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥(persist)í•©ë‹ˆë‹¤.\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=langchain_documents, \n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory # ì´ ê²½ë¡œì— DB íŒŒì¼ì´ ì €ì¥ë©ë‹ˆë‹¤.\n",
    "        )\n",
    "        print(f\"âœ… ChromaDBë¥¼ ìƒì„±í•˜ê³  '{persist_directory}' ê²½ë¡œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # 3. Retriever ìƒì„± (ì´ì „ê³¼ ì™„ë²½íˆ ë™ì¼)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    print(\"âœ… Retrieverë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ.\")\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1add9b-da27-4254-ac00-d6cbf5b337ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\n",
    "CSV_FILE_PATH = 'RTDB_test.csv'\n",
    "EMBEDDING_MODEL = 'nlpai-lab/KURE-v1'\n",
    "CHROMA_DB_PATH = 'chroma_db' # DB íŒŒì¼ì´ ì €ì¥ë  í´ë” ì´ë¦„\n",
    "# ChromaDBìš© í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "rag_retriever = setup_rag_pipeline_chroma(CSV_FILE_PATH, EMBEDDING_MODEL, CHROMA_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010a028-8615-46ab-afa6-b524db0f1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm ë©”ì¸ ëª¨ë¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97857f77-4393-48ca-8315-4ac149eacf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=\"GEMINI_API_KEY\",\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14617795-4792-4b0d-b3bc-590eb653e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import json\n",
    "import os\n",
    "from llama_cpp.llama_chat_format import Jinja2ChatFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f91b4a-f5d3-4fb6-a618-45305437283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Jinja ì±„íŒ… í…œí”Œë¦¿ íŒŒì¼ì—ì„œ ë¡œë“œ (ìˆ˜ì •ëœ ë¶€ë¶„) ---\n",
    "# 'lmalign_v1.jinja' íŒŒì¼ì´ ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n",
    "jinja_template_path = \"lmalign_v1.jinja\"\n",
    "\n",
    "if not os.path.exists(jinja_template_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"'{jinja_template_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. \"\n",
    "        \"ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ë””ë ‰í† ë¦¬ì— íŒŒì¼ì„ ìœ„ì¹˜ì‹œì¼œ ì£¼ì„¸ìš”.\"\n",
    "    )\n",
    "\n",
    "with open(jinja_template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    jinja_template = f.read()\n",
    "\n",
    "# --- 3. Jinja2ChatFormatter ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---\n",
    "chat_handler = Jinja2ChatFormatter(\n",
    "    template=jinja_template,\n",
    "    eos_token=\"<|eot_id|>\",\n",
    "    bos_token=\"<|begin_of_text|>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2a974-9865-4998-ba47-daa5be210cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Llama ëª¨ë¸ ë¡œë“œ ---\n",
    "# ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ê³¼ íŒŒì¼ì—ì„œ ì½ì–´ì˜¨ Jinja í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "llm = Llama(\n",
    "  model_path=model_path,\n",
    "  n_ctx=8192,         # Context window ì‚¬ì´ì¦ˆ\n",
    "  n_gpu_layers=-1,    # GPU ê°€ì† ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "  verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af9560-3511-4519-b776-e8b265e6b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. ì¶”ë¡  ì‹¤í–‰ ---\n",
    "# ì„œë¹„ìŠ¤ ì„¤ëª… ë° ì¶œë ¥ í†µì œ ì§€ì‹œì‚¬í•­ì„ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "#ì¶”ê°€ í”„ë¡¬: - ë°˜ë“œì‹œ JSON í¬ë§·ìœ¼ë¡œ ì‘ë‹µ\n",
    "\n",
    "## í‰ê°€ì llm ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "evaluation_system_prompt = \"\"\"\n",
    "ë‹¹ì‹ ì€ ê³ ë„ë¡œ ì „ë¬¸í™”ëœ íŠ¹í—ˆ ì‹¬ì‚¬ê´€ì´ì ê¸°ìˆ  ë¶„ì„ê°€ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì„ë¬´ëŠ” [ì‚¬ìš©ì ì•„ì´ë””ì–´]ì™€ [íŠ¹í—ˆ ë¬¸ì„œ ì¡°ê°]ì„ ì…ë ¥ë°›ì•„, ë‘ ë‚´ìš©ì˜ ê¸°ìˆ ì  ìœ ì‚¬ì„±ì„ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ë³´ê³ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¹ì‹ ì€ **ì ˆëŒ€** ì¼ë°˜ì ì¸ ëŒ€í™”í˜• í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µí•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ë‹¹ì‹ ì˜ **ìœ ì¼í•œ** ì‘ë‹µì€ `evaluation_idea` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "### âš–ï¸ í‰ê°€ ì§€ì¹¨\n",
    "\n",
    "ë‹¹ì‹ ì€ ë‹¤ìŒì˜ ì—„ê²©í•œ ê¸°ì¤€ì— ë”°ë¼ [ì‚¬ìš©ì ì•„ì´ë””ì–´]ì™€ [íŠ¹í—ˆ ë¬¸ì„œ ì¡°ê°]ì„ ë¹„êµí•˜ê³ , `evaluation_idea` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1.  **`eval_score` (0-100ì  ì‚¬ì´ì˜ ì •ìˆ˜):**\n",
    "    * `eval_score`ëŠ” **0ì ì—ì„œ 100ì  ì‚¬ì´ì˜ ëª¨ë“  ì •ìˆ˜(integer)**ë¡œ í‰ê°€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: 65ì , 82ì , 91ì )\n",
    "    * ì•„ë˜ì˜ ì ìˆ˜ ë²¤ì¹˜ë§ˆí¬ëŠ” **íŒë‹¨ì˜ ê¸°ì¤€ì„ (Benchmark)**ì…ë‹ˆë‹¤. ì´ ê¸°ì¤€ì„ **ì°¸ê³ **í•˜ì—¬, ë‘ ë‚´ìš©ì˜ ìœ ì‚¬ë„ì— ê°€ì¥ ì í•©í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” **êµ¬ì²´ì ì¸ ì •ìˆ˜ ì ìˆ˜**ë¥¼ ë„ì¶œí•˜ì„¸ìš”.\n",
    "\n",
    "    * **0ì  (ê¸°ì¤€):** [ì•„ì´ë””ì–´]ì™€ [íŠ¹í—ˆ ì¡°ê°]ì´ ê¸°ìˆ ì ìœ¼ë¡œ ì™„ì „íˆ ë¬´ê´€í•©ë‹ˆë‹¤.\n",
    "    * **25ì  (ì°¸ê³  ê¸°ì¤€):** ì¼ë¶€ í‚¤ì›Œë“œëŠ” ê²¹ì¹˜ë‚˜, í•µì‹¬ ê¸°ìˆ  ì›ë¦¬ë‚˜ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œê°€ ë‹¤ë¦…ë‹ˆë‹¤.\n",
    "    * **50ì  (ì¤‘ê°„ ê¸°ì¤€):** ìœ ì‚¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ í•˜ë‚˜, ì‚¬ìš©ëœ ê¸°ìˆ ì  ì ‘ê·¼ ë°©ì‹ì´ë‚˜ í•µì‹¬ êµ¬ì„±ì´ ìƒì´í•©ë‹ˆë‹¤.\n",
    "    * **75ì  (ì°¸K ê¸°ì¤€):** í•µì‹¬ ê¸°ìˆ  ì›ë¦¬ ë° ì ‘ê·¼ ë°©ì‹ì´ ë§¤ìš° ìœ ì‚¬í•˜ë©°, ì¼ë¶€ ì‚¬ì†Œí•œ êµ¬ì„± ìš”ì†Œë‚˜ ì ìš© ë²”ìœ„ì—ì„œë§Œ ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤.\n",
    "    * **100ì  (ê¸°ì¤€):** ë‘ ë‚´ìš©ì˜ í•µì‹¬ ê¸°ìˆ ì  ì‚¬ìƒ(ë°œëª…ì˜ ì›ë¦¬)ì´ ì‹¤ì§ˆì ìœ¼ë¡œ ë™ì¼í•˜ê±°ë‚˜ ëª…ë°±í•œ ë™ë“±ë¬¼ì…ë‹ˆë‹¤.\n",
    "\n",
    "2.  **`reason` (ë¬¸ìì—´):**\n",
    "    * í•´ë‹¹ `eval_score`ë¥¼ ë¶€ì—¬í•œ ëª…í™•í•˜ê³  ì „ë¬¸ê°€ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    * **[ë§¤ìš° ì¤‘ìš”]** [ì‚¬ìš©ì ì•„ì´ë””ì–´]ì˜ **'ì–´ë–¤ ê°œë…'**ì´ [íŠ¹í—ˆ ë¬¸ì„œ ì¡°ê°]ì˜ **'ì–´ëŠ íŠ¹ì • ë¶€ë¶„(ë¬¸êµ¬ ì¸ìš© ê°€ëŠ¥)'**ê³¼ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ ìœ ì‚¬í•œì§€ (ë˜ëŠ” ë‹¤ë¥¸ì§€) ë°˜ë“œì‹œ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    * ì¶”ìƒì ì¸ ì„¤ëª…ì´ ì•„ë‹Œ, êµ¬ì²´ì ì¸ ë¹„êµ ë¶„ì„ ë‚´ìš©ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "---\n",
    "\n",
    "ì´ì œ [ì‚¬ìš©ì ì•„ì´ë””ì–´]ì™€ [íŠ¹í—ˆ ë¬¸ì„œ ì¡°ê°]ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , ì¦‰ì‹œ `evaluation_idea` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c47b1e-d0a2-4a10-bdd8-62719acf3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Function Calling í…ŒìŠ¤íŠ¸ ---\n",
    "\n",
    "# 5-1. íŒŒì´ì¬ í•¨ìˆ˜ ì •ì˜\n",
    "# '-> float'ëŠ” ì´ í•¨ìˆ˜ê°€ ìˆ«ì(float)ë¥¼ ë°˜í™˜í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ëŠ” íƒ€ì… íŒíŠ¸ì…ë‹ˆë‹¤.\n",
    "def calculate_average(val1: float, val2: float) -> float:\n",
    "    \"\"\"ë‘ ê°œì˜ ìˆ«ì ê°’ì˜ ì‚°ìˆ  í‰ê· ì„ ê³„ì‚°í•˜ê³ , ê·¸ ìˆ«ì ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        average = (val1 + val2) / 2\n",
    "        print(f\"--- 'calculate_average' í˜¸ì¶œë¨ (val1={val1}, val2={val2}), ê²°ê³¼: {average} ---\")\n",
    "        return average # ğŸ‘ˆ ë¬¸ì¥ ëŒ€ì‹  ìˆ«ì ê°’ì„ ì§ì ‘ ë°˜í™˜\n",
    "    except TypeError:\n",
    "        return 0.0 # ì˜¤ë¥˜ ë°œìƒ ì‹œ 0.0 ë°˜í™˜\n",
    "\n",
    "# 5-2. ë„êµ¬(í•¨ìˆ˜) ëª©ë¡ ì •ì˜ (OpenAI tool-call í˜•ì‹)\n",
    "eval_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"evaluation_idea\",\n",
    "            \"description\": \"ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ì™€ íŠ¹í—ˆ ë¬¸ì„œ ì¡°ê°ì˜ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ì—¬, 0-100ì  ì‚¬ì´ì˜ ì ìˆ˜ì™€ ê·¸ ê·¼ê±°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"eval_score\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"ì‚¬ìš©ì ì•„ì´ë””ì–´ì™€ íŠ¹í—ˆ ì¡°ê° ê°„ì˜ ê¸°ìˆ ì  ìœ ì‚¬ë„ ì ìˆ˜. 0 (ì™„ì „íˆ ë¬´ê´€í•¨)ì—ì„œ 100 (ê¸°ìˆ ì ìœ¼ë¡œ ë™ì¼í•¨) ì‚¬ì´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤.\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 100\n",
    "                    },\n",
    "                    \"reason\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"í•´ë‹¹ ì ìˆ˜ë¥¼ ë¶€ì—¬í•œ êµ¬ì²´ì ì¸ ì´ìœ . íŠ¹í—ˆ ì¡°ê°ì˜ ì–´ëŠ ë¶€ë¶„ì´ ì•„ì´ë””ì–´ì˜ ì–´ë–¤ ê°œë…ê³¼ ìœ ì‚¬í•œì§€(ë˜ëŠ” ë‹¤ë¥¸ì§€) ëª…í™•íˆ ì§šì–´ì„œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"eval_score\", \"reason\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b903b-41f0-46f7-b9bf-42a330a652a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def execute_llm_turn(messages: list, max_depth: int = 5):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡(messages)ì„ ë°”íƒ•ìœ¼ë¡œ LLMì„ 1íšŒ í˜¸ì¶œí•˜ê³ ,\n",
    "    ê·¸ ê²°ê³¼ê°€ í•¨ìˆ˜ í˜¸ì¶œì´ë©´ ì¬ê·€ì ìœ¼ë¡œ ìì‹ ì„ ë‹¤ì‹œ í˜¸ì¶œí•˜ëŠ” í•µì‹¬ í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "    # 1. ì¬ê·€ í˜¸ì¶œì˜ ì•ˆì „ì¥ì¹˜: ìµœëŒ€ í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ë©´ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜\n",
    "    if max_depth <= 0:\n",
    "        return \"ìµœëŒ€ í•¨ìˆ˜ í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ì—¬ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "    # 2. ì£¼ì–´ì§„ messagesë¡œ LLM ì¶”ë¡  ì‹¤í–‰ (ì½”ë“œ ì¤‘ë³µ ì œê±°)\n",
    "    prompt_response = chat_handler(messages=messages, tools=tools)\n",
    "    response = llm.create_completion(\n",
    "        prompt=prompt_response.prompt,\n",
    "        max_tokens=2048,\n",
    "        stop=prompt_response.stop,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    reasoning_effort=\"medium\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": evaluation_system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"[ì‚¬ìš©ì ì•„ì´ë””ì–´]:{user_idea}\\n [íŠ¹í—ˆ ë¬¸ì„œ ì¡°ê°]: {do}\"\n",
    "        }\n",
    "    ],\n",
    "    tools=eval_tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "    output_text = response['choices'][0]['text'].strip()\n",
    "\n",
    "    # 3. LLMì˜ ì‘ë‹µì—ì„œ í•¨ìˆ˜ í˜¸ì¶œ íŒ¨í„´ í™•ì¸\n",
    "    match = re.search(r\"<function=.*?</function>\", output_text)\n",
    "\n",
    "    # 4. ì¬ê·€ì˜ ë¶„ê¸°ì : í•¨ìˆ˜ í˜¸ì¶œì´ ìˆëŠ”ì§€ ì—¬ë¶€\n",
    "    if not match:\n",
    "        # [ì¬ê·€ ì¢…ë£Œ ì¡°ê±´] í•¨ìˆ˜ í˜¸ì¶œì´ ì—†ìœ¼ë©´, í˜„ì¬ í…ìŠ¤íŠ¸ê°€ ìµœì¢… ë‹µë³€ì„\n",
    "        print(\"ğŸ¤– AI íŒë‹¨: ì¼ë°˜ ë‹µë³€ (ì¬ê·€ ì¢…ë£Œ)\")\n",
    "        return output_text\n",
    "    else:\n",
    "        # [ì¬ê·€ í˜¸ì¶œ ì¡°ê±´] í•¨ìˆ˜ í˜¸ì¶œì´ ìˆìœ¼ë©´, ì¶”ê°€ ì²˜ë¦¬ í›„ ìì‹ ì„ ë‹¤ì‹œ í˜¸ì¶œ\n",
    "        print(f\"ğŸ¤– AI íŒë‹¨: í•¨ìˆ˜ í˜¸ì¶œ í•„ìš” (ë‚¨ì€ í˜¸ì¶œ íšŸìˆ˜: {max_depth-1})\")\n",
    "        function_call_string = match.group(0)\n",
    "        \n",
    "        try:\n",
    "            # 4-1. í•¨ìˆ˜ ì‹¤í–‰ ë° ê²°ê³¼ ì–»ê¸°\n",
    "            func_name = function_call_string.split('>')[0].split('=')[1]\n",
    "            args_str = function_call_string.split('>')[1].split('<')[0]\n",
    "            args_json = json.loads(args_str)\n",
    "            \n",
    "            result_data = None\n",
    "            if func_name == \"calculate_average\":\n",
    "                result_data = calculate_average(**args_json)\n",
    "            \n",
    "            # 4-2. ë‹¤ìŒ ì¬ê·€ í˜¸ì¶œì„ ìœ„í•œ ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸\n",
    "            # ì´ì „ ê¸°ë¡ì— LLMì˜ í•¨ìˆ˜ í˜¸ì¶œ ì‘ë‹µê³¼ ì‹¤ì œ í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì¶”ê°€\n",
    "            updated_messages = messages + [\n",
    "                {\"role\": \"assistant\", \"content\": output_text},\n",
    "                {\"role\": \"ipython\", \"content\": str(result_data)}\n",
    "            ]\n",
    "            \n",
    "            # 4-3. ì—…ë°ì´íŠ¸ëœ ëŒ€í™” ê¸°ë¡ìœ¼ë¡œ ìê¸° ìì‹ ì„ ë‹¤ì‹œ í˜¸ì¶œ (ì¬ê·€)\n",
    "            return execute_llm_turn(messages=updated_messages, max_depth=max_depth - 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í•¨ìˆ˜ í˜¸ì¶œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            return \"í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "def get_single_turn_response(user_query: str):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ìì˜ ì´ˆê¸° ì§ˆë¬¸ì„ ë°›ì•„ ì¬ê·€ í•¨ìˆ˜ì˜ ì‹¤í–‰ì„ ì‹œì‘í•˜ëŠ” ì§„ì…ì  í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸš€ ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: \\\"{user_query}\\\"\")\n",
    "    # 1. ì‚¬ìš©ìì˜ ì²« ì§ˆë¬¸ìœ¼ë¡œ ì´ˆê¸° ëŒ€í™” ê¸°ë¡ ìƒì„±\n",
    "    initial_messages = [{\"role\": \"user\", \"content\": user_query}]\n",
    "    # 2. ì¬ê·€ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n",
    "    return execute_llm_turn(messages=initial_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ca0f3-f345-4490-b484-5402e3bcad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(user_query: str):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ RAGë¥¼ í†µí•´ ì°¾ì€ ì •ë³´ì™€ í•¨ê»˜ LLMì—ê²Œ ë‹µë³€ì„ ìš”ì²­í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- ì‚¬ìš©ì ì§ˆë¬¸: {user_query} ---\")\n",
    "    \n",
    "    # 1. Retrieve: RAG Retrieverë¡œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ë†’ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "    retrieved_docs = rag_retriever.invoke(user_query)\n",
    "    print(f\"ğŸ“š {len(retrieved_docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # 2. ì»¨í…ìŠ¤íŠ¸(Context) ìƒì„±: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹©ë‹ˆë‹¤.\n",
    "    context_data = \"\\n\".join([f\"- {doc.page_content}\" for doc in retrieved_docs])\n",
    "    '''\n",
    "    print(\"--- ê²€ìƒ‰ëœ RAG ë°ì´í„° ---\")\n",
    "    print(context_data)\n",
    "    print(\"-------------------------\")\n",
    "    '''\n",
    "    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±: ê¸°ì¡´ì˜ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ì§ˆë¬¸ê³¼ RAG ë°ì´í„°ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.\n",
    "    # ì´ëŠ” get_single_turn_response í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ,\n",
    "    # í•´ë‹¹ í•¨ìˆ˜ì— ì „ë‹¬í•  í˜•íƒœë¡œ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "    final_query_for_llm = f\"\"\"\n",
    "    ì•„ë˜ [ë°ì´í„°]ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì£¼ì„¸ìš”.\n",
    "    \n",
    "    [ì§ˆë¬¸]\n",
    "    {user_query}\n",
    "\n",
    "    [ë°ì´í„°]\n",
    "    {context_data}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 4. Generate: ì¬êµ¬ì„±ëœ ì§ˆë¬¸ì„ ê¸°ì¡´ì— ë§Œë“¤ì—ˆë˜ LLM í˜¸ì¶œ í•¨ìˆ˜ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    final_answer = get_single_turn_response(final_query_for_llm)\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede3a58-e16c-4479-be1f-c977f7f76dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ì‹¤ì œ ì„œë¹„ìŠ¤ ë¡œì§ ì˜ˆì‹œ (Stateless) ---\n",
    "\n",
    "# Case 1: ì¼ë°˜ì ì¸ íŒŒì‹±\n",
    "print(\"--- Case 1: í•¨ìˆ˜ í˜¸ì¶œ ì§ˆë¬¸ ---\")\n",
    "user_input_1 = \"\"\n",
    "final_response_1 = answer_with_rag(user_input_1)\n",
    "print(\"\\n[ìµœì¢… ë‹µë³€]\")\n",
    "print(final_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf790e-6ec9-4ed7-87fa-a262f0ad2a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
