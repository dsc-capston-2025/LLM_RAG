{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b720e-3853-4381-a6ca-1539cd9d1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm RAG 플러그인 설정(임베딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d923a60-c928-4f3c-8683-2e6464c0ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ede17c-965c-4439-ab04-29e26f6bc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import 수정\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document # LangChain의 기본 문서 형식을 import\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc75f3-cde9-44b8-8ed4-bf2ef8ecdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag_pipeline_chroma(file_path: str, embedding_model_name: str, persist_directory: str):\n",
    "    \"\"\"\n",
    "    CSV 파일로부터 ChromaDB RAG 파이프라인을 구축하고 검색기(Retriever)를 반환합니다.\n",
    "    \"\"\"\n",
    "    # 1. Embed: 임베딩 모델 로드 (이전과 동일)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    print(\"✅ 임베딩 모델을 로드했습니다.\")\n",
    "    \n",
    "    # 2. 벡터 스토어 생성 또는 로드\n",
    "    # persist_directory에 DB가 이미 존재하면 새로 만들지 않고 바로 로드합니다.\n",
    "    if os.path.exists(persist_directory):\n",
    "        print(\"✅ 기존 ChromaDB를 로드합니다.\")\n",
    "        vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    else:\n",
    "        print(\"⚠️ 기존 ChromaDB가 없으므로 새로 생성합니다.\")\n",
    "        # 1. Pandas로 CSV 파일 불러오기\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        print(f\"✅ Pandas로 CSV 파일을 불러왔습니다. (총 {len(df)}개 행)\")\n",
    "\n",
    "        # 2. 각 행을 자연어 문장으로 변환하고, LangChain 'Document' 객체로 만들기\n",
    "        langchain_documents = []\n",
    "        for timestamp, values in df.iterrows():\n",
    "            # 직접 만드신 자연어 변환 로직\n",
    "            sentence = f\"'{timestamp}'의 \"\n",
    "            value_strings = [f\"'{col}'는 {val}\" for col, val in values.items()]\n",
    "            sentence += \", \".join(value_strings)\n",
    "            \n",
    "            # 변환된 문장을 page_content로 하는 Document 객체 생성\n",
    "            # metadata에 원본 시간 정보를 넣어두면 나중에 유용하게 사용할 수 있습니다.\n",
    "            doc = Document(\n",
    "                page_content=sentence,\n",
    "                metadata={\"timestamp\": timestamp}\n",
    "            )\n",
    "            langchain_documents.append(doc)\n",
    "        \n",
    "        print(f\"✅ {len(langchain_documents)}개의 자연어 문서를 생성했습니다.\")\n",
    "        \n",
    "        # Store: ChromaDB를 생성하고 파일로 저장(persist)합니다.\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=langchain_documents, \n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory # 이 경로에 DB 파일이 저장됩니다.\n",
    "        )\n",
    "        print(f\"✅ ChromaDB를 생성하고 '{persist_directory}' 경로에 저장했습니다.\")\n",
    "\n",
    "    # 3. Retriever 생성 (이전과 완벽히 동일)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    print(\"✅ Retriever를 생성했습니다. RAG 파이프라인 준비 완료.\")\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1add9b-da27-4254-ac00-d6cbf5b337ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG 파이프라인 실행 ---\n",
    "CSV_FILE_PATH = 'RTDB_test.csv'\n",
    "EMBEDDING_MODEL = 'nlpai-lab/KURE-v1'\n",
    "CHROMA_DB_PATH = 'chroma_db' # DB 파일이 저장될 폴더 이름\n",
    "# ChromaDB용 함수를 호출합니다.\n",
    "rag_retriever = setup_rag_pipeline_chroma(CSV_FILE_PATH, EMBEDDING_MODEL, CHROMA_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010a028-8615-46ab-afa6-b524db0f1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm 메인 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97857f77-4393-48ca-8315-4ac149eacf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. GGUF 모델 다운로드 ---\n",
    "# Hugging Face Hub에서 지정된 모델 파일을 다운로드합니다.\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"jjkim110523/kanana-1.5-15.7b-a3b-instruct-Q4_K_M-GGUF\"\n",
    "model_file = \"kanana-1.5-15.7b-a3b-instruct-q4_k_m.gguf\"\n",
    "\n",
    "print(\"download!\")\n",
    "\n",
    "# progress bar와 함께 다운로드 진행\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name,\n",
    "    filename=model_file,\n",
    "    local_dir=\"./models\" # 다운로드 받을 로컬 경로 지정\n",
    ")\n",
    "\n",
    "print(f\"Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14617795-4792-4b0d-b3bc-590eb653e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import json\n",
    "import os\n",
    "from llama_cpp.llama_chat_format import Jinja2ChatFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f91b4a-f5d3-4fb6-a618-45305437283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Jinja 채팅 템플릿 파일에서 로드 (수정된 부분) ---\n",
    "# 'lmalign_v1.jinja' 파일이 이 스크립트와 같은 디렉토리에 있는지 확인하세요.\n",
    "jinja_template_path = \"lmalign_v1.jinja\"\n",
    "\n",
    "if not os.path.exists(jinja_template_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"'{jinja_template_path}' 파일을 찾을 수 없습니다. \"\n",
    "        \"스크립트와 동일한 디렉토리에 파일을 위치시켜 주세요.\"\n",
    "    )\n",
    "\n",
    "with open(jinja_template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    jinja_template = f.read()\n",
    "\n",
    "# --- 3. Jinja2ChatFormatter 인스턴스 생성 ---\n",
    "chat_handler = Jinja2ChatFormatter(\n",
    "    template=jinja_template,\n",
    "    eos_token=\"<|eot_id|>\",\n",
    "    bos_token=\"<|begin_of_text|>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de2a974-9865-4998-ba47-daa5be210cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Llama 모델 로드 ---\n",
    "# 다운로드한 모델과 파일에서 읽어온 Jinja 템플릿을 사용하여 모델을 초기화합니다.\n",
    "llm = Llama(\n",
    "  model_path=model_path,\n",
    "  n_ctx=8192,         # Context window 사이즈\n",
    "  n_gpu_layers=-1,    # GPU 가속 사용 (가능한 경우)\n",
    "  verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af9560-3511-4519-b776-e8b265e6b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. 추론 실행 ---\n",
    "# 서비스 설명 및 출력 통제 지시사항을 포함한 시스템 프롬프트 구성\n",
    "#추가 프롬: - 반드시 JSON 포맷으로 응답\n",
    "system_prompt = \"\"\"\n",
    "\n",
    "\n",
    "[함수(도구) 사용 규칙 (Function/Tool Usage Rules)]\n",
    "- 사용자의 질문을 해결할 수 있는 함수(도구)가 있다면, 반드시 해당 함수를 호출해야 합니다. 제공된 데이터에 정보가 없다고 해서 추측하여 답변하지 마세요.\n",
    "- 함수 실행 결과는 절대적인 사실(ground truth)로 간주하고, 절대 스스로 재계산하거나 검증하지 마세요. 주어진 결과를 그대로 사용하여 답변을 구성해야 합니다.\n",
    "- 함수를 호출하기 전, 대화 기록을 먼저 확인하여 동일한 함수를 동일한 매개변수로 이미 호출한 적이 있는지 검토해야 합니다. 만약 이전에 얻은 결과가 있다면, 절대 불필요하게 함수를 다시 호출하지 말고 기존 결과를 재사용하세요.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c47b1e-d0a2-4a10-bdd8-62719acf3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Function Calling 테스트 ---\n",
    "\n",
    "# 5-1. 파이썬 함수 정의\n",
    "# '-> float'는 이 함수가 숫자(float)를 반환한다는 것을 의미하는 타입 힌트입니다.\n",
    "def calculate_average(val1: float, val2: float) -> float:\n",
    "    \"\"\"두 개의 숫자 값의 산술 평균을 계산하고, 그 숫자 값을 반환합니다.\"\"\"\n",
    "    try:\n",
    "        average = (val1 + val2) / 2\n",
    "        print(f\"--- 'calculate_average' 호출됨 (val1={val1}, val2={val2}), 결과: {average} ---\")\n",
    "        return average # 👈 문장 대신 숫자 값을 직접 반환\n",
    "    except TypeError:\n",
    "        return 0.0 # 오류 발생 시 0.0 반환\n",
    "\n",
    "# 5-2. Kanana 모델에 알려줄 도구(함수) 목록 정의 (OpenAI tool-call 형식)\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_average\",\n",
    "            \"description\": \" task_D의 값을 구하는 함수로 주어진 정보에서 해당 시간대의 A값과 B을 찾아 평균을 계산합니다.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"val1\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"해당 시간대의 A 값입니다.\"\n",
    "                    },\n",
    "                    \"val2\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"해당 시간대의  B 값입니다.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"val1\", \"val2\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b903b-41f0-46f7-b9bf-42a330a652a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def execute_llm_turn(messages: list, max_depth: int = 5):\n",
    "    \"\"\"\n",
    "    주어진 대화 기록(messages)을 바탕으로 LLM을 1회 호출하고,\n",
    "    그 결과가 함수 호출이면 재귀적으로 자신을 다시 호출하는 핵심 함수.\n",
    "    \"\"\"\n",
    "    # 1. 재귀 호출의 안전장치: 최대 호출 횟수를 초과하면 에러 메시지 반환\n",
    "    if max_depth <= 0:\n",
    "        return \"최대 함수 호출 횟수를 초과하여 처리를 중단합니다.\"\n",
    "\n",
    "    # 2. 주어진 messages로 LLM 추론 실행 (코드 중복 제거)\n",
    "    prompt_response = chat_handler(messages=messages, tools=tools)\n",
    "    response = llm.create_completion(\n",
    "        prompt=prompt_response.prompt,\n",
    "        max_tokens=2048,\n",
    "        stop=prompt_response.stop,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    output_text = response['choices'][0]['text'].strip()\n",
    "\n",
    "    # 3. LLM의 응답에서 함수 호출 패턴 확인\n",
    "    match = re.search(r\"<function=.*?</function>\", output_text)\n",
    "\n",
    "    # 4. 재귀의 분기점: 함수 호출이 있는지 여부\n",
    "    if not match:\n",
    "        # [재귀 종료 조건] 함수 호출이 없으면, 현재 텍스트가 최종 답변임\n",
    "        print(\"🤖 AI 판단: 일반 답변 (재귀 종료)\")\n",
    "        return output_text\n",
    "    else:\n",
    "        # [재귀 호출 조건] 함수 호출이 있으면, 추가 처리 후 자신을 다시 호출\n",
    "        print(f\"🤖 AI 판단: 함수 호출 필요 (남은 호출 횟수: {max_depth-1})\")\n",
    "        function_call_string = match.group(0)\n",
    "        \n",
    "        try:\n",
    "            # 4-1. 함수 실행 및 결과 얻기\n",
    "            func_name = function_call_string.split('>')[0].split('=')[1]\n",
    "            args_str = function_call_string.split('>')[1].split('<')[0]\n",
    "            args_json = json.loads(args_str)\n",
    "            \n",
    "            result_data = None\n",
    "            if func_name == \"calculate_average\":\n",
    "                result_data = calculate_average(**args_json)\n",
    "            \n",
    "            # 4-2. 다음 재귀 호출을 위한 대화 기록 업데이트\n",
    "            # 이전 기록에 LLM의 함수 호출 응답과 실제 함수 실행 결과를 추가\n",
    "            updated_messages = messages + [\n",
    "                {\"role\": \"assistant\", \"content\": output_text},\n",
    "                {\"role\": \"ipython\", \"content\": str(result_data)}\n",
    "            ]\n",
    "            \n",
    "            # 4-3. 업데이트된 대화 기록으로 자기 자신을 다시 호출 (재귀)\n",
    "            return execute_llm_turn(messages=updated_messages, max_depth=max_depth - 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 함수 호출 처리 중 오류 발생: {e}\")\n",
    "            return \"함수를 실행하는 중에 오류가 발생했습니다.\"\n",
    "\n",
    "def get_single_turn_response(user_query: str):\n",
    "    \"\"\"\n",
    "    사용자의 초기 질문을 받아 재귀 함수의 실행을 시작하는 진입점 함수.\n",
    "    \"\"\"\n",
    "    print(f\"\\n🚀 새 질문 처리 시작: \\\"{user_query}\\\"\")\n",
    "    # 1. 사용자의 첫 질문으로 초기 대화 기록 생성\n",
    "    initial_messages = [{\"role\": \"user\", \"content\": user_query}]\n",
    "    # 2. 재귀 함수를 호출하여 전체 프로세스 시작\n",
    "    return execute_llm_turn(messages=initial_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ca0f3-f345-4490-b484-5402e3bcad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(user_query: str):\n",
    "    \"\"\"\n",
    "    사용자 질문에 대해 RAG를 통해 찾은 정보와 함께 LLM에게 답변을 요청하는 함수\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 사용자 질문: {user_query} ---\")\n",
    "    \n",
    "    # 1. Retrieve: RAG Retriever로 사용자 질문과 가장 관련 높은 문서를 검색합니다.\n",
    "    retrieved_docs = rag_retriever.invoke(user_query)\n",
    "    print(f\"📚 {len(retrieved_docs)}개의 관련 문서를 찾았습니다.\")\n",
    "\n",
    "    # 2. 컨텍스트(Context) 생성: 검색된 문서들의 내용을 하나의 문자열로 합칩니다.\n",
    "    context_data = \"\\n\".join([f\"- {doc.page_content}\" for doc in retrieved_docs])\n",
    "    '''\n",
    "    print(\"--- 검색된 RAG 데이터 ---\")\n",
    "    print(context_data)\n",
    "    print(\"-------------------------\")\n",
    "    '''\n",
    "    # 3. 프롬프트 구성: 기존의 프롬프트 형식에 질문과 RAG 데이터를 결합합니다.\n",
    "    # 이는 get_single_turn_response 함수 내부에서 처리되므로,\n",
    "    # 해당 함수에 전달할 형태로 질문을 재구성합니다.\n",
    "    final_query_for_llm = f\"\"\"\n",
    "    아래 [데이터]를 참고하여 질문에 대답해주세요.\n",
    "    \n",
    "    [질문]\n",
    "    {user_query}\n",
    "\n",
    "    [데이터]\n",
    "    {context_data}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 4. Generate: 재구성된 질문을 기존에 만들었던 LLM 호출 함수에 전달합니다.\n",
    "    final_answer = get_single_turn_response(final_query_for_llm)\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede3a58-e16c-4479-be1f-c977f7f76dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 실제 서비스 로직 예시 (Stateless) ---\n",
    "\n",
    "# Case 1: 일반적인 파싱\n",
    "print(\"--- Case 1: 함수 호출 질문 ---\")\n",
    "user_input_1 = \"\"\n",
    "final_response_1 = answer_with_rag(user_input_1)\n",
    "print(\"\\n[최종 답변]\")\n",
    "print(final_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf790e-6ec9-4ed7-87fa-a262f0ad2a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_venv",
   "language": "python",
   "name": "jupyter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
